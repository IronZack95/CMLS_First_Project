\subsection{SVM Application}

We finally arrive at the core of the machine learning classification process, the training of the model. First we have to do some considerations about the chosen of Machine type needed for classification problem. Like said before our intent is to make a pyramid scheme of binary decision distinguishing step by step which class an input belong to, so we need a chain of binary classificator machines, which under supervised learning process, are trained to best separated input features from a class to another.\\
We choose in particular a simple structured machines called Support Vector Machines, or SVM, witch best fit that purpose.\\
A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.\\
SVM capable to perform a binary or multi-class  classification on a Dataset are also called C-Support Vector Classification or SVC.\\
We implement this model using \texttt{scikit} library for python using the function \texttt{clf = svm.SVC()}.\\
The matematical rappresentation of SVC is the following:\\
Given training vectors $x_i \in \mathbb{R}^p $, i=1,…, n, in two classes, and a vector $y \in \{1, -1\}^n$, our goal is to find $w \in \mathbb{R}^p $ such that the prediction given by \text{sign} $(w^T\phi(x) + b)$ is correct for most samples.\\
SVC solves the following problem:
\begin{align}\begin{aligned}\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n} \zeta_i\\\begin{split}\textrm {subject to } & y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
& \zeta_i \geq 0, i=1, ..., n\end{split}\end{aligned}\end{align}
Intuitively, we’re trying to maximize the margin (by minimizing $||w||^2 = w^Tw$), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value $y_i (w^T \phi (x_i) + b)$ would be $\geq 1$ for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance $\zeta_i$ from their correct margin boundary.\\
The penalty term C controls the strengh of this penalty, and as a result, acts as an inverse regularization parameter. Another important parameter is $\phi (x_i)$ also called kernel it determine the shape of the hiperplane that divides features, the impact on the division is showed clearly in the Figure \ref{fig5}.
\begin{figure}[h!]
\centering
\includegraphics[scale = 0.8]{svc_kernel.png}
\caption{Example of different kernel impact on SVC}
\label{fig5}
\end{figure}\\
For setting up CVS we can interact with two main parameters:\\
\begin{itemize}
    \item Set penality term C = 1
    \item Set kernel $\phi (x_i)$ = 'rbf', means Gaussian shape.
\end{itemize}
Once the optimization problem is solved, the output of  \textbf{decision function} for a given sample $x$ becames:
\begin{align}\begin{aligned}\sum_{i\in SV} y_i \alpha_i K(x_i, x) + b,\end{aligned}\end{align}
and the predicted class correspond to its sign. We only need to sum over the support vectors (i.e. the samples that lie within the margin) because the dual coefficients $\alpha_i$ are zero for the other samples. After being fitted, the model can then be used to predict new values with the function \texttt{clf.predict()}, which in binary classification case, set the predicted output to 1 if the prediction probability is greater than 0.5 and 0 viceversa. \\
